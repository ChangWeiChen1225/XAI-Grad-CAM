{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4519694,"sourceType":"datasetVersion","datasetId":2641391},{"sourceId":4519803,"sourceType":"datasetVersion","datasetId":2641442},{"sourceId":5808194,"sourceType":"datasetVersion","datasetId":3336162},{"sourceId":5808437,"sourceType":"datasetVersion","datasetId":3336303},{"sourceId":6159848,"sourceType":"datasetVersion","datasetId":3533121},{"sourceId":6753419,"sourceType":"datasetVersion","datasetId":3887686}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport numpy as np\n# from tensorflow.keras import layers, models, Sequential, Conv2D, MaxPooling2D, Dropout\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dense, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, concatenate, Input\nimport matplotlib.pyplot as plt","metadata":{"id":"7MLC-A5viJx6","execution":{"iopub.status.busy":"2023-11-14T07:56:30.416818Z","iopub.execute_input":"2023-11-14T07:56:30.417271Z","iopub.status.idle":"2023-11-14T07:56:35.875095Z","shell.execute_reply.started":"2023-11-14T07:56:30.417180Z","shell.execute_reply":"2023-11-14T07:56:35.873997Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport cv2\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.applications import imagenet_utils\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as preprocess_vgg19\nfrom matplotlib import pyplot as plt\nimport numpy as np","metadata":{"id":"PJjXNSwtD-xx","execution":{"iopub.status.busy":"2023-11-14T07:56:38.059225Z","iopub.execute_input":"2023-11-14T07:56:38.059910Z","iopub.status.idle":"2023-11-14T07:56:38.511201Z","shell.execute_reply.started":"2023-11-14T07:56:38.059870Z","shell.execute_reply":"2023-11-14T07:56:38.510314Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tensorflow._api.v2.compat.v1 import float32\n# # Load DREAMS data and devide into 2 class\n# import cv2\n# from google.colab.patches import cv2_imshow\n# import os\n# excerpt_data_path = r'/content/drive/MyDrive/DREAMS_data/SMOTE_5s_based_regenerated_balanced_data/compare_1D&2D/SMOTE_5s_epoch_based_with_without_REMs_phase_shift_80%_overlap_plot'\n# excerpts_data_folder_name = os.listdir(excerpt_data_path)\n\n# # load data\n# img_size = (128, 128)\n# RGB_img_size = (128, 128, 3)\n# with_REMs_img = np.zeros(RGB_img_size, dtype='float32')\n# without_REMs_img = np.zeros(RGB_img_size, dtype='float32')\n# with_REMs_img = np.expand_dims(with_REMs_img, axis=0)\n# without_REMs_img = np.expand_dims(without_REMs_img, axis=0)\n\n\n# for folder_name in excerpts_data_folder_name:\n#   img_names = os.listdir(excerpt_data_path + '/' + folder_name)\n#   # print(img_names)\n#   for name in img_names:\n#     if name[-5] == '1':\n#       # print(name)\n#       img = cv2.imread(excerpt_data_path + '/' + folder_name + '/' + name)\n#       img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n#       img = np.expand_dims(img, axis=0)\n#       with_REMs_img = np.append(with_REMs_img, img, axis=0)\n#       print(with_REMs_img.shape)\n#     elif name[-5] == '0':\n#       # print(name)\n#       img = cv2.imread(excerpt_data_path + '/' + folder_name + '/' + name)\n#       img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n#       img = np.expand_dims(img, axis=0)\n#       without_REMs_img = np.append(without_REMs_img, img, axis=0)\n#       print(without_REMs_img.shape)\n\n# with_REMs_img = preprocess_resnet50(with_REMs_img)\n# without_REMs_img = preprocess_resnet50(without_REMs_img)\n# # print('Summary')\n# # print(with_REMs_img.shape)\n# # print(without_REMs_img.shape)\n# # with_REMs_img = np.delete(with_REMs_img, 0, axis=0)\n# # without_REMs_img = np.delete(without_REMs_img, 0, axis=0)\n# # print(with_REMs_img.shape)\n# # print(without_REMs_img.shape)\n\n","metadata":{"id":"8wn0IqELDhzP","outputId":"5d171dc8-9a21-4789-ea21-dd7a2f234e0c","scrolled":true,"execution":{"iopub.status.busy":"2023-09-16T23:36:23.390584Z","iopub.execute_input":"2023-09-16T23:36:23.390997Z","iopub.status.idle":"2023-09-16T23:36:23.397283Z","shell.execute_reply.started":"2023-09-16T23:36:23.390960Z","shell.execute_reply":"2023-09-16T23:36:23.396259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('with REMs')\nprint(with_REMs_img)\nprint('without REMs')\nprint(without_REMs_img)","metadata":{"id":"7n7G6C-Vdlfi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('with REMs')\nfor i in range(0,60):\n  cv2_imshow(with_REMs_img[20+i,:,:,:])\n  print('')\n\nprint('without REMs')\nfor i in range(0,60):\n  cv2_imshow(without_REMs_img[20+i,:,:,:])\n  print('')","metadata":{"id":"xMd1q7k83CX3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shuffle divided-into-two-class data\nfrom sklearn.utils import shuffle\nwith_REMs_img = shuffle(with_REMs_img, random_state=0)\nwithout_REMs_img = shuffle(without_REMs_img, random_state=0)\nwith_REMs_img_amount = with_REMs_img.shape[0]\nselected_img = np.append(with_REMs_img, without_REMs_img[:with_REMs_img_amount,:,:,:], axis=0)\nselected_img_label = np.append(np.ones((with_REMs_img_amount,), dtype='int'), np.zeros((with_REMs_img_amount,), dtype='int'))\nprint(selected_img.shape)\nprint(selected_img)\nprint(selected_img_label[1:30])\nselected_img, selected_img_label = shuffle(selected_img, selected_img_label, random_state=0)\nprint(selected_img_label[1:30])","metadata":{"id":"ekJoxRDtd_ks","outputId":"51d16c00-712a-4748-f559-616bf4655c82","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0,60):\n  cv2_imshow(selected_img[20+i,:,:,:])\n  print('')","metadata":{"id":"tRcJg6AwxQXw","outputId":"3832fd59-448d-4241-b928-e7a862e55add","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DREAMS data(balance data) partition\ntrain_amount = int(selected_img.shape[0] * 80/100)\nvalid_amount = int(selected_img.shape[0] * 10/100)\ntest_amount = int(selected_img.shape[0] * 10/100)\ntrain_images = selected_img[0:train_amount,:,:,:]\ntrain_labels = selected_img_label[0:train_amount]\nvalid_images = selected_img[train_amount:train_amount + valid_amount,:,:,:]\nvalid_labels = selected_img_label[train_amount:train_amount + valid_amount]\ntest_images = selected_img[train_amount + valid_amount:train_amount + valid_amount + test_amount,:,:,:]\ntest_labels = selected_img_label[train_amount + valid_amount:train_amount + valid_amount + test_amount]","metadata":{"id":"tX1AVbeZjVvZ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0,60):\n  cv2_imshow(test_images[20+i,:,:,:])\n  print('')","metadata":{"id":"OnnjqEm0xefn","outputId":"e274acc8-1dd5-45ba-e947-890401828876","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Coherence heatmap data","metadata":{}},{"cell_type":"code","source":"# Load DREAMS data(with/without REMs)(whole)\nimport cv2\n# from google.colab.patches import cv2_imshow\nimport os\n# from natsort import natsorted\nexcerpt_data_path = r'/kaggle/input/5s-rems-slidingwindow-noise-augment-coherence-plot/5s_epoch_based_with_without_REMs_sliding_window_whiteNoise_augmented_coherence_plot/5s_epoch_based_with_without_REMs_sliding_window_whiteNoise_augmented_coherence_plot'\nexcerpts_data_folder_name = os.listdir(excerpt_data_path)\nexcerpts_data_folder_name = sorted(excerpts_data_folder_name)\n\n\n# load data\nimg_size = (128, 128)\n\n\n#Subjects data start_index & end_index mark\nsubjects_data_index_mark = np.zeros((len(excerpts_data_folder_name),2), dtype='int')\nsubject_count = 0\nindex = 0\nimg_count = 0\n\nimg_names = os.listdir(excerpt_data_path + '/' + excerpts_data_folder_name[0])\n# img_names = sorted(img_names)\n# print(img_names[0])\nsubjects_data_index_mark[subject_count, 0] = index\nimg_count += 1\nif img_names[0][-5] == '1':\n    labels = 1\nelif img_names[0][-5] == '0':\n    labels = 0\nelse:\n    print(img_names[0][-5])\nlabels = np.expand_dims(labels, axis=0)  \nprint(labels.shape)\nimages = cv2.imread(excerpt_data_path + '/' + excerpts_data_folder_name[0] + '/' + img_names[0])\n# cv2_imshow(images) \nimages = cv2.resize(images, img_size, interpolation=cv2.INTER_AREA)\n# cv2_imshow(images)\n# print(images)\nimages = np.expand_dims(images, axis=0)\nprint(images.shape)\nprint(labels.shape)\n# print(preprocess_images.shape)\nfor name in img_names[1:]:\n#     print(name)\n    img_count += 1\n    if name[-5] == '1':\n        label = 1\n    elif name[-5] == '0':\n        label = 0\n    else:\n        print(name[-5])\n    label = np.expand_dims(label, axis=0)  \n    print(labels.shape)\n    print(label.shape)\n    labels = np.append(labels, label, axis = 0)\n    img = cv2.imread(excerpt_data_path + '/' + excerpts_data_folder_name[0] + '/' + name)\n    img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n    img = np.expand_dims(img, axis=0)\n    images = np.append(images, img, axis=0)\n    print(images.shape)\n    print(labels.shape)\nsubjects_data_index_mark[subject_count, 1] = index + img_count\nindex = index + img_count\nsubject_count += 1\n    \n\nfor folder_name in excerpts_data_folder_name[1:]:\n    img_names = os.listdir(excerpt_data_path + '/' + folder_name)\n#     img_names = sorted(img_names)\n    subjects_data_index_mark[subject_count, 0] = index\n    img_count = 0\n    for name in img_names[0:]:\n        print(name)\n        img_count += 1\n        if name[-5] == '1':\n            label = 1\n        elif name[-5] == '0':\n            label = 0\n        else:\n            print(name[-5])\n        label = np.expand_dims(label, axis=0)\n        labels = np.append(labels, label, axis = 0)\n        img = cv2.imread(excerpt_data_path + '/' + folder_name + '/' + name)\n        img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n        img = np.expand_dims(img, axis=0)\n        images = np.append(images, img, axis=0)\n        print(images.shape)\n        print(labels.shape)\n    subjects_data_index_mark[subject_count, 1] = index + img_count\n    index = index + img_count\n    subject_count += 1\n\nprint(subjects_data_index_mark)\n# preprocess_images = preprocess_resnet50(images)\n# print(preprocess_images)\n# print(preprocess_images.shape)\n","metadata":{"id":"GmzTtfIQ2JJx","outputId":"e1736304-1b96-485d-de3e-cafd6cce182b","scrolled":true,"execution":{"iopub.status.busy":"2023-11-14T07:56:47.256571Z","iopub.execute_input":"2023-11-14T07:56:47.256977Z","iopub.status.idle":"2023-11-14T08:01:30.758304Z","shell.execute_reply.started":"2023-11-14T07:56:47.256941Z","shell.execute_reply":"2023-11-14T08:01:30.757298Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T14:28:22.608446Z","iopub.execute_input":"2023-09-01T14:28:22.608967Z","iopub.status.idle":"2023-09-01T14:28:22.622868Z","shell.execute_reply.started":"2023-09-01T14:28:22.608914Z","shell.execute_reply":"2023-09-01T14:28:22.621678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 透過convert_to_labels 將one-hot-encode的資料轉成label\ndef convert_to_labels(X):\n  return np.argmax(X, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T14:28:28.182480Z","iopub.execute_input":"2023-09-01T14:28:28.182889Z","iopub.status.idle":"2023-09-01T14:28:28.188156Z","shell.execute_reply.started":"2023-09-01T14:28:28.182854Z","shell.execute_reply":"2023-09-01T14:28:28.187117Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Leave-one-subject-out cross validation","metadata":{}},{"cell_type":"code","source":"#LOSO validation\n# from tensorflow.keras.applications.vgg19 import preprocess_input as preprocess_vgg19\n# from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\nfrom sklearn.utils import shuffle\n\n# Define per-subject-left-out score containers\nacc_per_subject_left_out = []\nloss_per_subject_left_out = []\n\n\n# Spectrogram CNN model parameters\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\n# #preprocess\n# #     preprocess_images = np.log(images + 1e-10)\n# preprocess_images = preprocess_resnet50(images)\n\ndef LOSO_cross_validation():\n    for i in range(0, len(excerpts_data_folder_name)): \n        \n\n        #leave on subject out as test data\n        img_infront_test_subject = images[:subjects_data_index_mark[i,0],:,:,:]\n        print(img_infront_test_subject.shape)\n        label_infront_test_subject = labels[:subjects_data_index_mark[i,0]]\n        img_behind_test_subject = images[subjects_data_index_mark[i,1]:,:,:,:]\n        print(img_behind_test_subject.shape)\n        label_behind_test_subject = labels[subjects_data_index_mark[i,1]:]\n\n    #     if(label_infront_test_subject.shape[0]==0):\n    #         training_img = img_behind_test_subject\n    #         training_labels = label_behind_test_subject\n    #     elif(img_behind_test_subject.shape[0]==0):\n    #         training_img = label_infront_test_subject\n    #         training_labels = label_infront_test_subject\n    #     else:\n    #         training_img = np.concatenate([img_infront_test_subject, img_behind_test_subject], axis=0)\n    #         training_labels = np.concatenate([label_infront_test_subject, label_behind_test_subject], axis=0)\n        training_img = np.concatenate([img_infront_test_subject, img_behind_test_subject], axis=0)\n        training_labels = np.concatenate([label_infront_test_subject, label_behind_test_subject], axis=0)\n        test_img = images[subjects_data_index_mark[i,0]:subjects_data_index_mark[i,1],:,:,:]\n        test_labels = labels[subjects_data_index_mark[i,0]:subjects_data_index_mark[i,1]]\n        \n        # z score transform\n        mean_value = training_img.mean()\n        std_value = training_img.std()\n        print(mean_value)\n        print(std_value)\n        print(training_img.shape)\n        for j in range(training_img.shape[0]):\n            training_img[j] = (training_img[j]-mean_value) / std_value\n\n        print(test_img.shape)\n        for j in range(test_img.shape[0]):\n            test_img[j] = (test_img[j]-mean_value) / std_value\n\n        #shuffle\n        training_images,training_labels= shuffle(training_img, training_labels, random_state=0)\n        test_images,test_labels= shuffle(test_img, test_labels, random_state=0)\n\n        #validation data partition?\n\n        #統計兩類數量\n#         print(training_images.shape)\n#     #     print(valid_images.shape)\n#         print(test_images.shape)\n#         print(training_labels.shape)\n#     #     print(valid_labels.shape)\n#         print(test_labels.shape)\n#         train_count = 0\n#     #     valid_count = 0\n#         test_count = 0\n#         for i in range(len(training_labels)):\n#           if(training_labels[i]==1):\n#             train_count += 1\n#     #     for i in range(len(valid_labels)):\n#     #       if(valid_labels[i]==1):\n#     #         valid_count += 1\n#         for i in range(len(test_labels)):\n#           if(test_labels[i]==1):\n#             test_count += 1\n\n#         print(train_count)\n#     #     print(valid_count)\n#         print(test_count)\n\n\n        #Construct model\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n        model.add(tf.keras.layers.Conv2D(4, 3, strides=2, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dense(16, activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.5))\n        model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n        # model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n\n        # Compile model\n        model.compile(\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n            optimizer='adam',\n    #         optimizer='SGD',\n        #     optimizer=tf.keras.optimizers.legacy.RMSprop(),\n            metrics=['accuracy'],\n        )\n\n        #star training\n        history = model.fit(\n            training_images,\n            training_labels,\n            epochs=200,\n            batch_size=64,\n    #         callbacks=[lr_sched],\n            validation_data=(test_images, test_labels)\n        )\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-11T00:54:58.113049Z","iopub.execute_input":"2023-09-11T00:54:58.113597Z","iopub.status.idle":"2023-09-11T00:54:58.187878Z","shell.execute_reply.started":"2023-09-11T00:54:58.113551Z","shell.execute_reply":"2023-09-11T00:54:58.186638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating the time consuming\nimport timeit\n\nprint(timeit.timeit(LOSO_cross_validation, number=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-11T00:55:01.612737Z","iopub.execute_input":"2023-09-11T00:55:01.613574Z","iopub.status.idle":"2023-09-11T01:20:24.388512Z","shell.execute_reply.started":"2023-09-11T00:55:01.613530Z","shell.execute_reply":"2023-09-11T01:20:24.387237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#LOSO validation\n# from tensorflow.keras.applications.vgg19 import preprocess_input as preprocess_vgg19\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\nfrom sklearn.utils import shuffle\n\n# Define per-subject-left-out score containers\nacc_per_subject_left_out = []\nloss_per_subject_left_out = []\n\n\n# Spectrogram CNN model parameters\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import regularizers\n###########################################\n# DEFINE A STEPPED LEARNING RATE SCHEDULE #\n###########################################\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))\n\n\n#data partition\nfor i in range(0, len(excerpts_data_folder_name)): \n    #preprocess\n#     preprocess_images = np.log(images + 1e-10)\n    preprocess_images = preprocess_resnet50(images)\n    \n    #leave on subject out as test data\n    img_infront_test_subject = preprocess_images[:subjects_data_index_mark[i,0],:,:,:]\n    print(img_infront_test_subject.shape)\n    label_infront_test_subject = labels[:subjects_data_index_mark[i,0]]\n    img_behind_test_subject = preprocess_images[subjects_data_index_mark[i,1]:,:,:,:]\n    print(img_behind_test_subject.shape)\n    label_behind_test_subject = labels[subjects_data_index_mark[i,1]:]\n    \n#     if(label_infront_test_subject.shape[0]==0):\n#         training_img = img_behind_test_subject\n#         training_labels = label_behind_test_subject\n#     elif(img_behind_test_subject.shape[0]==0):\n#         training_img = label_infront_test_subject\n#         training_labels = label_infront_test_subject\n#     else:\n#         training_img = np.concatenate([img_infront_test_subject, img_behind_test_subject], axis=0)\n#         training_labels = np.concatenate([label_infront_test_subject, label_behind_test_subject], axis=0)\n    training_img = np.concatenate([img_infront_test_subject, img_behind_test_subject], axis=0)\n    training_labels = np.concatenate([label_infront_test_subject, label_behind_test_subject], axis=0)\n    test_img = preprocess_images[subjects_data_index_mark[i,0]:subjects_data_index_mark[i,1],:,:,:]\n    test_labels = labels[subjects_data_index_mark[i,0]:subjects_data_index_mark[i,1]]\n    \n    \n    #shuffle\n    training_images,training_labels= shuffle(training_img, training_labels, random_state=0)\n    test_images,test_labels= shuffle(test_img, test_labels, random_state=0)\n    \n    #validation data partition?\n    \n    #統計兩類數量\n    print(training_images.shape)\n#     print(valid_images.shape)\n    print(test_images.shape)\n    print(training_labels.shape)\n#     print(valid_labels.shape)\n    print(test_labels.shape)\n    train_count = 0\n#     valid_count = 0\n    test_count = 0\n    for i in range(len(training_labels)):\n      if(training_labels[i]==1):\n        train_count += 1\n#     for i in range(len(valid_labels)):\n#       if(valid_labels[i]==1):\n#         valid_count += 1\n    for i in range(len(test_labels)):\n      if(test_labels[i]==1):\n        test_count += 1\n\n    print(train_count)\n#     print(valid_count)\n    print(test_count)\n    \n    \n    #Construct model\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n    model.add(tf.keras.layers.Conv2D(4, 3, strides=2, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(16, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n    # model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n\n    # Compile model\n    model.compile(\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        optimizer='adam',\n#         optimizer='SGD',\n    #     optimizer=tf.keras.optimizers.legacy.RMSprop(),\n        metrics=['accuracy'],\n    )\n    \n    #star training\n    history = model.fit(\n        training_images,\n        training_labels,\n        epochs=200,\n        batch_size=128,\n#         callbacks=[lr_sched],\n        validation_data=(test_images, test_labels)\n    )\n    \n    \n    # Generate generalization metrics\n    scores = model.evaluate(test_images, test_labels, verbose=0)\n    print(f'Score for subject{i+1} left out: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_subject_left_out.append(scores[1] * 100)\n    loss_per_subject_left_out.append(scores[0])\n    \n    \n    #Plot training, validation and test set accuracy\n    print(\"Training Curve\")\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    #Plot training, validation and test set loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n    \n    \n    \n    #  Generate confudion matrix(main function)\n    y_predict = model.predict(test_images, batch_size=None, verbose=0, steps=None)\n#     print(y_predict)\n    y_pred = convert_to_labels(y_predict)\n#     print(y_pred)\n    y_true = test_labels\n    target_names = ['No_spindle','With_spindle']\n    print(classification_report(y_true, y_pred, target_names=target_names))\n    print (\"**************************************************************\")\n    plt.figure()\n    cnf_matrix = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=False, title='confusion matrix')\n    plt.show()\n    \n    \n    \n\n\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per subject')\nfor i in range(0, len(acc_per_subject_left_out)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_subject_left_out[i]} - Accuracy: {acc_per_subject_left_out[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all subjects:')\nprint(f'> Accuracy: {np.mean(acc_per_subject_left_out)} (+- {np.std(acc_per_subject_left_out)})')\nprint(f'> Loss: {np.mean(loss_per_subject_left_out)}')\nprint('------------------------------------------------------------------------')\n    \n    \n    \n    \n    \n    \n    \n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-21T05:22:19.512077Z","iopub.execute_input":"2023-10-21T05:22:19.512908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Try with resnet50 to compare with our modedl","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\npreprocess_images = preprocess_resnet50(images)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T05:22:06.870103Z","iopub.execute_input":"2023-10-21T05:22:06.870903Z","iopub.status.idle":"2023-10-21T05:22:07.248636Z","shell.execute_reply.started":"2023-10-21T05:22:06.870865Z","shell.execute_reply":"2023-10-21T05:22:07.247741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preprocess (log scaling)\n\nprint(images)\nlog_scaled_image = np.log(images + 1e-10)\nprint(log_scaled_image)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-20T13:50:45.941737Z","iopub.execute_input":"2023-10-20T13:50:45.942089Z","iopub.status.idle":"2023-10-20T13:50:47.753115Z","shell.execute_reply.started":"2023-10-20T13:50:45.942062Z","shell.execute_reply":"2023-10-20T13:50:47.752156Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import cv2\n# from google.colab.patches import cv2_imshow\n# import os\n# excerpt_data_path = r'/content/drive/MyDrive/DREAMS_data/SMOTE_5s_based_regenerated_balanced_data/compare_1D&2D/SMOTE_5s_epoch_based_with_without_REMs_phase_shift_80%_overlap_plot'\n# excerpts_data_folder_name = os.listdir(excerpt_data_path)\n# print(excerpts_data_folder_name)\n# from natsort import natsorted\n# print(natsorted(excerpts_data_folder_name))\n# print(excerpts_data_folder_name)\n# import os\n# excerpt_label_path = r'/content/drive/MyDrive/DREAMS_data/SMOTE_5s_based_regenerated_balanced_data/compare_1D&2D/SMOTE_5s_epoch_based_with_without_REMs_80%_overlap_label'\n# excerpt_label_txt_name = os.listdir(excerpt_label_path)\n# print(excerpt_label_txt_name)\n# from natsort import natsorted\n# print(natsorted(excerpt_label_txt_name))\n# print(excerpt_label_txt_name)","metadata":{"id":"UHh0lZ0oOsIB","outputId":"afcda51f-3cb3-4f19-8e6c-6cae8ec74d54"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"# shuffle DREAMS data\n\nfrom sklearn.utils import shuffle\nprint(labels[:30])\npreprocess_images,labels = shuffle(preprocess_images, labels, random_state=0)\n# randomize = np.arange(len(labels_int))\n# np.random.shuffle(randomize)\n# x = x[randomize]\n# y = y[randomize]\nprint(labels[:30])","metadata":{"id":"H2YiFgRvEEL6","outputId":"c2666a77-b60e-4543-ca55-c9eb4dc201e3","execution":{"iopub.status.busy":"2023-10-21T03:53:50.397355Z","iopub.execute_input":"2023-10-21T03:53:50.398201Z","iopub.status.idle":"2023-10-21T03:53:51.889633Z","shell.execute_reply.started":"2023-10-21T03:53:50.398166Z","shell.execute_reply":"2023-10-21T03:53:51.888597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shuffle DREAMS data\n\nfrom sklearn.utils import shuffle\nprint(labels[:30])\nimages,labels = shuffle(images, labels, random_state=0)\n# randomize = np.arange(len(labels_int))\n# np.random.shuffle(randomize)\n# x = x[randomize]\n# y = y[randomize]\nprint(labels[:30])","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:03:03.210027Z","iopub.execute_input":"2023-11-14T08:03:03.211061Z","iopub.status.idle":"2023-11-14T08:03:04.000161Z","shell.execute_reply.started":"2023-11-14T08:03:03.211021Z","shell.execute_reply":"2023-11-14T08:03:03.999117Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DREAMS data(stage1,2)/label(stage1) whole partition(train:valid:test = 0.8:0.1:0.1)\ntrain_amount = int(log_scaled_image.shape[0] * 80/100)\nvalid_amount = int(log_scaled_image.shape[0] * 10/100)\ntest_amount = int(log_scaled_image.shape[0] * 10/100)\ntrain_images = log_scaled_image[0:train_amount,:,:,:]\ntrain_labels = labels[0:train_amount]\nvalid_images = log_scaled_image[train_amount:train_amount + valid_amount,:,:,:]\nvalid_labels = labels[train_amount:train_amount + valid_amount]\ntest_images = log_scaled_image[train_amount + valid_amount:train_amount + valid_amount + test_amount,:,:,:]\ntest_labels = labels[train_amount + valid_amount:train_amount + valid_amount + test_amount]","metadata":{"id":"wR9FlWonS85R","execution":{"iopub.status.busy":"2023-09-01T13:53:48.296667Z","iopub.execute_input":"2023-09-01T13:53:48.297044Z","iopub.status.idle":"2023-09-01T13:53:48.657548Z","shell.execute_reply.started":"2023-09-01T13:53:48.297004Z","shell.execute_reply":"2023-09-01T13:53:48.656061Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DREAMS data(stage1,2)/label(stage1) whole partition(train:valid:test = 0.8:0.1:0.1)\ntrain_amount = int(preprocess_images.shape[0] * 80/100)\nvalid_amount = int(preprocess_images.shape[0] * 10/100)\ntest_amount = int(preprocess_images.shape[0] * 10/100)\ntrain_images = preprocess_images[0:train_amount,:,:,:]\ntrain_labels = labels[0:train_amount]\nvalid_images = preprocess_images[train_amount:train_amount + valid_amount,:,:,:]\nvalid_labels = labels[train_amount:train_amount + valid_amount]\ntest_images = preprocess_images[train_amount + valid_amount:train_amount + valid_amount + test_amount,:,:,:]\ntest_labels = labels[train_amount + valid_amount:train_amount + valid_amount + test_amount]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:14:38.270078Z","iopub.execute_input":"2023-10-21T04:14:38.270980Z","iopub.status.idle":"2023-10-21T04:14:38.278965Z","shell.execute_reply.started":"2023-10-21T04:14:38.270941Z","shell.execute_reply":"2023-10-21T04:14:38.277802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DREAMS data(stage1,2)/label(stage1) whole partition(train:valid:test = 0.8:0.1:0.1)\ntrain_amount = int(images.shape[0] * 80/100)\nvalid_amount = int(images.shape[0] * 10/100)\ntest_amount = int(images.shape[0] * 10/100)\ntrain_images = images[0:train_amount,:,:,:]\ntrain_labels = labels[0:train_amount]\nvalid_images = images[train_amount:train_amount + valid_amount,:,:,:]\nvalid_labels = labels[train_amount:train_amount + valid_amount]\ntest_images = images[train_amount + valid_amount:train_amount + valid_amount + test_amount,:,:,:]\ntest_labels = labels[train_amount + valid_amount:train_amount + valid_amount + test_amount]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:03:11.369010Z","iopub.execute_input":"2023-11-14T08:03:11.369431Z","iopub.status.idle":"2023-11-14T08:03:11.377961Z","shell.execute_reply.started":"2023-11-14T08:03:11.369396Z","shell.execute_reply":"2023-11-14T08:03:11.376848Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# z score normalization\ntrain_images = train_images.astype('float64')\nvalid_images = valid_images.astype('float64')\ntest_images = test_images.astype('float64')\nprint(train_images.max())\nprint(train_images.min())\n\nprint(valid_images.max())\nprint(valid_images.min())\n\nprint(test_images.max())\nprint(test_images.min())\n\nprint(train_images.shape)\nmean_value = train_images.mean()\nstd_value = train_images.std()\nprint(mean_value)\nprint(std_value)\nfor i in range(train_images.shape[0]):\n    train_images[i,:,:,:] = (train_images[i,:,:,:]-mean_value) / std_value\n            \n# mean_value = valid_data.mean()\n# std_value = valid_data.std()\n# print(mean_value)\n# print(std_value)\nprint(valid_images.shape)\nfor i in range(valid_images.shape[0]):\n    valid_images[i,:,:,:] = (valid_images[i,:,:,:]-mean_value) / std_value\n            \n# mean_value = test_data.mean()\n# std_value = test_data.std()\n# print(mean_value)\n# print(std_value)  \nprint(test_images.shape)\nfor i in range(test_images.shape[0]):\n    test_images[i,:,:,:] = (test_images[i,:,:,:]-mean_value) / std_value\n            \nprint(train_images.max())\nprint(train_images.min())\n\nprint(valid_images.max())\nprint(valid_images.min())\n\nprint(test_images.max())\nprint(test_images.min())","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:03:15.664664Z","iopub.execute_input":"2023-11-14T08:03:15.665071Z","iopub.status.idle":"2023-11-14T08:03:18.521381Z","shell.execute_reply.started":"2023-11-14T08:03:15.665036Z","shell.execute_reply":"2023-11-14T08:03:18.520351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_images)","metadata":{"id":"y1IEwHASRikB","outputId":"8d7a2315-c79c-45ad-c48c-a4ab89d25696","scrolled":true,"execution":{"iopub.status.busy":"2023-10-22T05:59:22.072493Z","iopub.execute_input":"2023-10-22T05:59:22.073326Z","iopub.status.idle":"2023-10-22T05:59:22.087836Z","shell.execute_reply.started":"2023-10-22T05:59:22.073276Z","shell.execute_reply":"2023-10-22T05:59:22.086755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_images.shape)\nprint(valid_images.shape)\nprint(train_labels.shape)\nprint(valid_labels.shape)\ntrain_count = 0\nvalid_count = 0\ntest_count = 0\nfor i in range(len(train_labels)):\n  if(train_labels[i]==1):\n    train_count += 1\nfor i in range(len(valid_labels)):\n  if(valid_labels[i]==1):\n    valid_count += 1\nfor i in range(len(valid_labels)):\n  if(test_labels[i]==1):\n    test_count += 1\n\nprint(train_count)\nprint(valid_count)\nprint(test_count)","metadata":{"id":"vRm6a3hsbpmG","outputId":"d13845fa-02ed-47b9-cca3-d3573419c143","execution":{"iopub.status.busy":"2023-11-14T08:03:24.668580Z","iopub.execute_input":"2023-11-14T08:03:24.669512Z","iopub.status.idle":"2023-11-14T08:03:24.681158Z","shell.execute_reply.started":"2023-11-14T08:03:24.669473Z","shell.execute_reply":"2023-11-14T08:03:24.679975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tuning model for training","metadata":{}},{"cell_type":"code","source":"kernel_size2 = (5, 5)\nkernel_size = (3, 3)\nmodel = Sequential()\nmodel.call = tf.function(model.call)\n# The first two layers with 32 filters of window size 3x3\nmodel.add(Conv2D(32, kernel_size, padding='same', activation='relu', input_shape=(256, 256, 3)))\nmodel.add(Conv2D(32, kernel_size, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size, padding='same', activation='relu'))\nmodel.add(Conv2D(64, kernel_size, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size, padding='same', activation='relu'))\nmodel.add(Conv2D(64, kernel_size, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","metadata":{"id":"oPMx5hGmnjL-","outputId":"bda98fdc-3eda-4c0e-959f-bef09bcf135f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',classes=np.unique(train_labels),y=train_labels)\n\nhistory = model.fit(train_images, train_labels, epochs=20, class_weight=class_weight,\n                    validation_data=(valid_images, valid_labels))","metadata":{"id":"4XzRtXd-nuot","outputId":"afeaae82-e24d-41b3-fcd1-e36b960c9867"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ResNet50\nfrom tensorflow.keras.applications import ResNet50\n\nnum_classes = 2    \npred_model = ResNet50(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling='max', classifier_activation='softmax')\noutput_layer = Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\nmodel = tf.keras.Model(pred_model.inputs, output_layer(pred_model.output))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',classes=np.unique(train_labels),y=train_labels)\n\nhistory = model.fit(train_images, train_labels, epochs=20, class_weight=class_weight,\n                    validation_data=(valid_images, valid_labels))","metadata":{"id":"sJ78lyoNWX-v","outputId":"78dcf271-41ed-45c0-b259-5ac1bbd9a691"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow==2.7.0\n!pip install tf-models-official==2.7.0\n!pip install tensorflow_io==0.23.1","metadata":{"id":"kS93r6T-0Thl","outputId":"23c4bbcd-f417-4dec-f6f2-0c8258d31576","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Try with VGG19 for compare","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG19\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\n# load the pretrained model\nbase_model = tf.keras.applications.VGG19(\n        input_shape=(256, 256, 3),\n        include_top=False,\n        weights=\"imagenet\",\n)\n\n# freeze all layers of the model\nbase_model.trainable = False\n\n# # we start by unfreezing all layers of the base model\n# base_model.trainable = True\n\n# # Freeze all layers except the 10 last layers \n# for layer in base_model.layers[:-10]: \n#     layer.trainable = False\n\n# learning_rate = 1e-4\n\n# # compile and retrain with a low learning rate\n# # low_lr = learning_rate / 10\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(256, 256, 3)),\n    # layers.Rescaling(scale=1.0/255), # rescale the pixel values to the [0, 1] range\n    base_model,\n    # add a new FC classifier on top of the base model\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\nlearning_rate = 1e-4\nmodel.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n              metrics=['accuracy']\n             )\n\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',classes=np.unique(train_labels),y=train_labels)\n\nepochs = 20\nhistory = model.fit(train_images, train_labels, epochs=epochs, class_weight=class_weight,\n                   validation_data=(valid_images, valid_labels))","metadata":{"id":"Ce8jcMODUC0l","outputId":"2c5a4cda-696c-4e3d-d762-4bb212763b17"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_images.max())\nprint(train_images.min())","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:03:36.382597Z","iopub.execute_input":"2023-11-14T08:03:36.383385Z","iopub.status.idle":"2023-11-14T08:03:36.664717Z","shell.execute_reply.started":"2023-11-14T08:03:36.383349Z","shell.execute_reply":"2023-11-14T08:03:36.663644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Try with our target model","metadata":{}},{"cell_type":"code","source":"# Spectrogram CNN model\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import regularizers\n###########################################\n# DEFINE A STEPPED LEARNING RATE SCHEDULE #\n###########################################\n\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\nmodel.add(tf.keras.layers.Conv2D(4, 3, strides=2, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Flatten())\n# model.add(tf.keras.layers.Dense(4, activation='relu'))\n# model.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n# model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n\n# Compile model\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    optimizer='adam',\n#     optimizer='SGD',\n#     optimizer=tf.keras.optimizers.RMSprop(),\n    metrics=['accuracy'],\n)\n\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',classes=np.unique(train_labels),y=train_labels)\n\n# history = model.fit(train_images, train_labels, epochs=100, batch_size=32, callbacks=[lr_sched], class_weight=class_weight, validation_data=(valid_images, valid_labels))\nhistory = model.fit(train_images, train_labels, epochs=100, batch_size=256, \n#                     callbacks=[lr_sched], \n                    validation_data=(valid_images, valid_labels))","metadata":{"id":"OzaLq6aXM1jq","outputId":"38c4e265-3fee-442d-f1bf-e05903f675b6","scrolled":true,"execution":{"iopub.status.busy":"2023-11-14T08:03:41.544772Z","iopub.execute_input":"2023-11-14T08:03:41.545886Z","iopub.status.idle":"2023-11-14T08:05:07.516562Z","shell.execute_reply.started":"2023-11-14T08:03:41.545836Z","shell.execute_reply":"2023-11-14T08:05:07.515703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:05:35.929717Z","iopub.execute_input":"2023-11-14T08:05:35.930588Z","iopub.status.idle":"2023-11-14T08:05:35.938386Z","shell.execute_reply.started":"2023-11-14T08:05:35.930545Z","shell.execute_reply":"2023-11-14T08:05:35.937294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.plot(history.history['accuracy'], label='accuracy')\n# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.ylim([0.5, 1])\n# plt.legend(loc='lower right')\n\n# test_loss, test_acc = model.evaluate(valid_images, valid_labels, verbose=2)\n# print(train_images)\n\n#Plot training, validation and test set accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n#Plot training, validation and test set loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"id":"n5bFPKdKnv_P","outputId":"44060914-5e1e-4339-acfc-21bbd5506f2b","execution":{"iopub.status.busy":"2023-11-14T08:05:40.297777Z","iopub.execute_input":"2023-11-14T08:05:40.298568Z","iopub.status.idle":"2023-11-14T08:05:40.757088Z","shell.execute_reply.started":"2023-11-14T08:05:40.298530Z","shell.execute_reply":"2023-11-14T08:05:40.756128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#10 fold cross validation\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\n\n# Spectrogram CNN model parameter\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\ninputs = np.concatenate((train_images, valid_images), axis=0)\ntargets = np.concatenate((train_labels, valid_labels), axis=0)\n\n\n# loss_function = sparse_categorical_crossentropy\n# no_epochs = 25\n# optimizer = Adam()\n# # verbosity = 1\nnum_folds = 10\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n\nepochs=200\nbatch_size=64\n\n\npreprocess_images = preprocess_resnet50(images)\ndef ten_folds_cross_validation():\n    fold_no = 1\n    for train, test in kfold.split(inputs, targets):\n        \n      # Define the model architecture\n      #Construct model\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n        model.add(tf.keras.layers.Conv2D(4, 3, strides=2, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dense(16, activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.5))\n        model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n        # model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n\n        # Compile model\n        model.compile(\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n            optimizer='adam',\n    #         optimizer='SGD',\n        #     optimizer=tf.keras.optimizers.legacy.RMSprop(),\n            metrics=['accuracy'],\n        )\n\n        # Generate a print\n        print('------------------------------------------------------------------------')\n        print(f'Training for fold {fold_no} ...')\n\n        # Fit data to model\n        history = model.fit(inputs[train], targets[train],\n                      batch_size=batch_size,\n                      epochs=epochs)\n\n        # Increase fold number\n        fold_no = fold_no + 1\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-11T02:39:16.703022Z","iopub.execute_input":"2023-09-11T02:39:16.703390Z","iopub.status.idle":"2023-09-11T02:39:17.323665Z","shell.execute_reply.started":"2023-09-11T02:39:16.703356Z","shell.execute_reply":"2023-09-11T02:39:17.322623Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating the time consuming\nimport timeit\n\nprint(timeit.timeit(ten_folds_cross_validation, number=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-11T02:39:21.149782Z","iopub.execute_input":"2023-09-11T02:39:21.150521Z","iopub.status.idle":"2023-09-11T03:02:13.986083Z","shell.execute_reply.started":"2023-09-11T02:39:21.150480Z","shell.execute_reply":"2023-09-11T03:02:13.984994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1 prediction\ndef one_prediction():\n    y_predict = model.predict(test_images, batch_size=None, verbose=0, steps=None)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T00:38:27.559835Z","iopub.execute_input":"2023-09-17T00:38:27.560607Z","iopub.status.idle":"2023-09-17T00:38:27.566688Z","shell.execute_reply.started":"2023-09-17T00:38:27.560559Z","shell.execute_reply":"2023-09-17T00:38:27.565615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1 prediction time\nimport timeit\n\nprint(timeit.timeit(one_prediction, number=1000))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T00:40:49.575991Z","iopub.execute_input":"2023-09-17T00:40:49.576718Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#10 fold cross validation\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\n\n# Spectrogram CNN model parameter\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import regularizers\n###########################################\n# DEFINE A STEPPED LEARNING RATE SCHEDULE #\n###########################################\n\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))\n\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\ninputs = np.concatenate((train_images, valid_images), axis=0)\ntargets = np.concatenate((train_labels, valid_labels), axis=0)\n\n\n# loss_function = sparse_categorical_crossentropy\n# no_epochs = 25\n# optimizer = Adam()\n# # verbosity = 1\nnum_folds = 10\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n\nepochs=200\nbatch_size=64\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold.split(inputs, targets):\n    \n    \n  # Define the model architecture\n  #Construct model\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n    model.add(tf.keras.layers.Conv2D(4, 3, strides=2, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(16, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n    # model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n\n    # Compile model\n    model.compile(\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        optimizer='adam',\n#         optimizer='SGD',\n    #     optimizer=tf.keras.optimizers.legacy.RMSprop(),\n        metrics=['accuracy'],\n    )\n    \n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n    # Fit data to model\n    history = model.fit(inputs[train], targets[train],\n                  batch_size=batch_size,\n                  epochs=epochs)\n    \n    \n    \n    #  Generate confudion matrix(main function)\n    y_predict = model.predict(inputs[test], batch_size=None, verbose=0, steps=None)\n#     print(y_predict)\n    y_pred = convert_to_labels(y_predict)\n#     print(y_pred)\n    y_true = targets[test]\n    target_names = ['No_spindle','With_spindle']\n    print(classification_report(y_true, y_pred, target_names=target_names))\n    print (\"**************************************************************\")\n    plt.figure()\n    cnf_matrix = confusion_matrix(y_true, y_pred)\n    plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=False, title='confusion matrix')\n    plt.show()\n    \n    \n    # Generate generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    # Increase fold number\n    fold_no = fold_no + 1\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n    print('------------------------------------------------------------------------')\n    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","metadata":{"id":"GMTw8SuEqCCc","outputId":"0adc101e-e955-4636-ee88-f9ee4e7aaa16","execution":{"iopub.status.busy":"2023-09-01T14:38:24.799963Z","iopub.execute_input":"2023-09-01T14:38:24.800427Z","iopub.status.idle":"2023-09-01T15:10:44.030852Z","shell.execute_reply.started":"2023-09-01T14:38:24.800386Z","shell.execute_reply":"2023-09-01T15:10:44.026797Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the performance of testing data\nresults = model.evaluate(test_images,test_labels)\nprint(model.metrics_names)\nprint(results)\n\npreds = model.predict(test_images[1:2,:,:,:])\nprint(preds)","metadata":{"id":"ERwG8pqNErcL","outputId":"fc7375d9-095c-438c-bf43-b25965d2387d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"id":"4gZvG6n2Rlni","outputId":"0eddcb0e-9298-4636-d37e-0361b1369a6f","execution":{"iopub.status.busy":"2023-07-24T01:19:45.783952Z","iopub.execute_input":"2023-07-24T01:19:45.784348Z","iopub.status.idle":"2023-07-24T01:19:45.794282Z","shell.execute_reply.started":"2023-07-24T01:19:45.784313Z","shell.execute_reply":"2023-07-24T01:19:45.793226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:31:01.149607Z","iopub.execute_input":"2023-07-24T01:31:01.149990Z","iopub.status.idle":"2023-07-24T01:31:01.217770Z","shell.execute_reply.started":"2023-07-24T01:31:01.149958Z","shell.execute_reply":"2023-07-24T01:31:01.216721Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 透過convert_to_labels 將one-hot-encode的資料轉成label\ndef convert_to_labels(X):\n  return np.argmax(X, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:31:03.932698Z","iopub.execute_input":"2023-07-24T01:31:03.933080Z","iopub.status.idle":"2023-07-24T01:31:03.938783Z","shell.execute_reply.started":"2023-07-24T01:31:03.933046Z","shell.execute_reply":"2023-07-24T01:31:03.937527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Generate confudion matrix(main function)\ny_predict = model.predict(test_images, batch_size=None, verbose=0, steps=None)\n# print(y_predict)\ny_pred = convert_to_labels(y_predict)\n# print(y_pred)\ny_true = test_labels\ntarget_names = ['No_REMs','With_REMs']\nprint(classification_report(y_true, y_pred, target_names=target_names))\nprint (\"**************************************************************\")\n\n\n\n\n\n\n\n\nplt.figure()\ncnf_matrix = confusion_matrix(y_true, y_pred)\nplot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True, title='confusion matrix')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:31:28.227242Z","iopub.execute_input":"2023-07-24T01:31:28.227649Z","iopub.status.idle":"2023-07-24T01:31:28.996244Z","shell.execute_reply.started":"2023-07-24T01:31:28.227613Z","shell.execute_reply":"2023-07-24T01:31:28.995093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, img_label, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n\n    if(pred_index==1):\n        if(img_label==1):\n            result = 'Groundtruth: REMs/ Prediction: REMs'\n        elif(img_label):\n            result = 'Groundtruth: NoREMs/ Prediction: REMs'\n    elif(pred_index==0):\n        if(img_label==1):\n            result = 'Groundtruth: REMs/ Prediction: NoREMs'\n        elif(img_label==0):\n            result = 'Groundtruth: NoREMs/ Prediction: NoREMs'\n\n\n    return heatmap.numpy(),result","metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:19:32.636587Z","iopub.execute_input":"2023-07-24T01:19:32.637762Z","iopub.status.idle":"2023-07-24T01:19:32.652979Z","shell.execute_reply.started":"2023-07-24T01:19:32.637714Z","shell.execute_reply":"2023-07-24T01:19:32.651709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"misclassify_count = 0\ncorrect_record = np.zeros((300,), dtype=int)\nfor i in range(0,493):\n  test_img_matrix = test_images[i:i+1,:,:,:]\n  test_img = test_images[i,:,:,:]\n  test_img_label = test_labels[i,]\n  heatmap, result = make_gradcam_heatmap(test_img_matrix, test_img_label, model, 'conv2d_5')\n  # if (result == 'Groundtruth: NoSpindle/ Prediction: Spindle') | (result == 'Groundtruth: Spindle/ Prediction: NoSpindle'):\n  if (result == 'Groundtruth: REMs/ Prediction: REMs'):\n#     misclassify_count += 1\n    print(heatmap.shape)\n    print(i)\n    correct_record[i] = 1\n    # Display heatmap\n    plt.matshow(heatmap)\n    plt.show()\n\n\n    heatmap = cv2.resize(heatmap,(test_img_matrix.shape[1],test_img_matrix.shape[2]), interpolation=cv2.INTER_CUBIC)\n    # heatmap = np.uint8(255 * heatmap)\n    fig, axs = plt.subplots(1, 1, sharex=True, sharey=True)\n    test_img = np.float32(test_img)\n    print(test_img.dtype)\n    im = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n#     cv2.imshow(\"windows_name\", im)\n    # 以 0.6 透明度繪製原始影像\n    print(result)\n    print('')\n    axs.imshow(im, alpha=0.6)   \n    # 以 0.4 透明度繪製熱力圖\n#     axs.imshow(heatmap, cmap='jet', alpha=0.4)\n  if (result == 'Groundtruth: REMs/ Prediction: NoREMs'):\n    misclassify_count += 1\n    print(heatmap.shape)\n    print(i)\n    correct_record[i] = 1\n    # Display heatmap\n    plt.matshow(heatmap)\n    plt.show()\n\n\n    heatmap = cv2.resize(heatmap,(test_img_matrix.shape[1],test_img_matrix.shape[2]), interpolation=cv2.INTER_CUBIC)\n    # heatmap = np.uint8(255 * heatmap)\n    fig, axs = plt.subplots(1, 1, sharex=True, sharey=True)\n    test_img = np.float32(test_img)\n    print(test_img.dtype)\n    im = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n#     cv2.imshow(\"windows_name\", im)\n    # 以 0.6 透明度繪製原始影像\n    print(result)\n    print('')\n    axs.imshow(im, alpha=0.6)   \n    # 以 0.4 透明度繪製熱力圖\n#     axs.imshow(heatmap, cmap='jet', alpha=0.4)\nprint(misclassify_count)\nfor index, value in enumerate(correct_record):\n  if value == 1:\n    print(index)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:27:00.639600Z","iopub.execute_input":"2023-07-24T01:27:00.640056Z","iopub.status.idle":"2023-07-24T01:27:08.931778Z","shell.execute_reply.started":"2023-07-24T01:27:00.640012Z","shell.execute_reply":"2023-07-24T01:27:08.930005Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load DREAMS original data(no augment & shuffle)\nimport cv2\n# from google.colab.patches import cv2_imshow\nimport os\n# from natsort import natsorted\nexcerpt_data_path = r'/kaggle/input/5-sec-based-rems-regenerated-2d-data/5s_epoch_based_with_without_REMs_phase_shift_plot'\nexcerpts_data_folder_name = os.listdir(excerpt_data_path)\nexcerpts_data_folder_name = sorted(excerpts_data_folder_name)\n\npositive_class_index = np.zeros((3240,), dtype=int)\nimg_count = 0\n# load data\nimg_size = (128, 128)\n\nimg_count = 0\n\nimg_names = os.listdir(excerpt_data_path + '/' + excerpts_data_folder_name[0])\n# img_names = sorted(img_names)\n# print(img_names[0])\n\nif img_names[0][-5] == '1':\n    labels = 1\n#     positive_class_index[img_count] = \nelif img_names[0][-5] == '0':\n    labels = 0\nelse:\n    print(img_names[0][-5])\nimg_count+=1\nlabels = np.expand_dims(labels, axis=0)  \nprint(labels.shape)\nimgs = cv2.imread(excerpt_data_path + '/' + excerpts_data_folder_name[0] + '/' + img_names[0])\n# cv2_imshow(images) \nimgs = cv2.resize(imgs, img_size, interpolation=cv2.INTER_AREA)\n# cv2_imshow(images)\n# print(images)\nimgs = np.expand_dims(imgs, axis=0)\nprint(images.shape)\nprint(labels.shape)\n# print(preprocess_images.shape)\nfor name in img_names[1:]:\n#     print(name)\n    if name[-5] == '1':\n        label = 1\n    elif name[-5] == '0':\n        label = 0\n    else:\n        print(name[-5])\n    label = np.expand_dims(label, axis=0)  \n    print(labels.shape)\n    print(label.shape)\n    labels = np.append(labels, label, axis = 0)\n    img = cv2.imread(excerpt_data_path + '/' + excerpts_data_folder_name[0] + '/' + name)\n    img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n    img = np.expand_dims(img, axis=0)\n    imgs = np.append(imgs, img, axis=0)\n    print(imgs.shape)\n    print(labels.shape)\n    \n\nfor folder_name in excerpts_data_folder_name[1:]:\n    img_names = os.listdir(excerpt_data_path + '/' + folder_name)\n#     img_names = sorted(img_names)\n    for name in img_names[0:]:\n        print(name)\n        if name[-5] == '1':\n            label = 1\n        elif name[-5] == '0':\n            label = 0\n        else:\n            print(name[-5])\n        label = np.expand_dims(label, axis=0)\n        labels = np.append(labels, label, axis = 0)\n        img = cv2.imread(excerpt_data_path + '/' + folder_name + '/' + name)\n        img = cv2.resize(img, img_size, interpolation=cv2.INTER_AREA)\n        img = np.expand_dims(img, axis=0)\n        imgs = np.append(imgs, img, axis=0)\n        print(imgs.shape)\n        print(labels.shape)\n\n# original_preprocess_images = preprocess_resnet50(images)\n\noriginal_preprocess_images = imgs.astype('float64')\n\n# z score normalization\nfor i in range(original_preprocess_images.shape[0]):\n    original_preprocess_images[i,:,:,:] = (original_preprocess_images[i,:,:,:]-mean_value) / std_value\n\noriginal_labels = labels\n# print(preprocess_images)\n# print(preprocess_images.shape)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-14T08:05:54.522434Z","iopub.execute_input":"2023-11-14T08:05:54.523124Z","iopub.status.idle":"2023-11-14T08:08:12.847404Z","shell.execute_reply.started":"2023-11-14T08:05:54.523090Z","shell.execute_reply":"2023-11-14T08:08:12.846423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del img,imgs","metadata":{"execution":{"iopub.status.busy":"2023-10-22T09:14:44.341712Z","iopub.execute_input":"2023-10-22T09:14:44.342635Z","iopub.status.idle":"2023-10-22T09:14:44.347023Z","shell.execute_reply.started":"2023-10-22T09:14:44.342594Z","shell.execute_reply":"2023-10-22T09:14:44.345983Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(original_preprocess_images)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T06:33:43.077748Z","iopub.execute_input":"2023-10-22T06:33:43.078468Z","iopub.status.idle":"2023-10-22T06:33:43.094669Z","shell.execute_reply.started":"2023-10-22T06:33:43.078428Z","shell.execute_reply":"2023-10-22T06:33:43.093506Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(original_preprocess_images.max())\nprint(original_preprocess_images.min())","metadata":{"execution":{"iopub.status.busy":"2023-11-14T08:11:18.915784Z","iopub.execute_input":"2023-11-14T08:11:18.916252Z","iopub.status.idle":"2023-11-14T08:11:19.153605Z","shell.execute_reply.started":"2023-11-14T08:11:18.916213Z","shell.execute_reply":"2023-11-14T08:11:19.152480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(original_labels[2000:3000])","metadata":{"execution":{"iopub.status.busy":"2023-10-21T05:17:21.187005Z","iopub.execute_input":"2023-10-21T05:17:21.187426Z","iopub.status.idle":"2023-10-21T05:17:21.196239Z","shell.execute_reply.started":"2023-10-21T05:17:21.187393Z","shell.execute_reply":"2023-10-21T05:17:21.195301Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor i in range(2000,3000):\n    preds = model.predict(original_preprocess_images[i:i+1,:,:,:])\n    pred_class = np.argmax(preds)\n    print(pred_class)\n    if((pred_class == original_labels[i]) and (original_labels[i]==1)):\n        print(\"true positive sample\")\n        print(i)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-22T06:34:07.024951Z","iopub.execute_input":"2023-10-22T06:34:07.025889Z","iopub.status.idle":"2023-10-22T06:34:43.936723Z","shell.execute_reply.started":"2023-10-22T06:34:07.025846Z","shell.execute_reply":"2023-10-22T06:34:43.935770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(images)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T06:48:23.168285Z","iopub.execute_input":"2023-10-22T06:48:23.169391Z","iopub.status.idle":"2023-10-22T06:48:23.178031Z","shell.execute_reply.started":"2023-10-22T06:48:23.169354Z","shell.execute_reply":"2023-10-22T06:48:23.177053Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2d-cnn grad-cam \nfrom tensorflow.keras import models\nimport tensorflow.keras.backend as K\nimport cv2\nimport keras\nimport torch\nimport tensorflow.compat.v1 as tf\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n\nlast_conv_layer = model.get_layer(index=-7)\nheatmap_model = models.Model([model.input], [last_conv_layer.output, model.output])\n\nfor i in range(1000,1100):\n    input_test_img = original_preprocess_images[i:i+1,:,:,:]\n    preds = model.predict(input_test_img)\n    pred_class = np.argmax(preds)\n    if((pred_class == original_labels[i]) and (original_labels[i]==1)):\n        print(\"true positive sample\")\n        print(i)\n        with tf.GradientTape() as gtape:\n            conv_output, Predictions = heatmap_model(input_test_img)\n            prob = Predictions[:, K.argmax(Predictions[0])]\n            grads = gtape.gradient(prob, conv_output)\n            pooled_grads = K.mean(grads, axis=(0,1,2)) \n  \n\n        heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n        print(heatmap.shape)\n        heatmap = K.maximum(heatmap,0)\n        print(heatmap.shape)\n        max_heat = K.max(heatmap)\n        if max_heat == 0:\n            max_heat = 1e-10\n        heatmap /= max_heat\n        \n        heatmap_original = np.asarray(heatmap)\n        \n        plt.imshow(255*heatmap[0])\n  \n    \n  \n#         print(heatmap)\n        im = cv2.resize(cv2.cvtColor(imgs[i,:,:,:], cv2.COLOR_BGR2RGB), (imgs[i,:,:,:].shape[1], imgs[i,:,:,:].shape[0]))\n    \n        # 拉伸 heatmap\n        heatmap = cv2.resize(np.asarray(heatmap[0]), (imgs[i,:,:,:].shape[1], imgs[i,:,:,:].shape[0]))\n\n        heatmap = np.uint8(255 * heatmap)\n\n\n  \n        \n\n        # 分別plot         \n        time = [0,1,2,3,4,5]\n        default_x_ticks = [0,25.6,51.2,76.8,102.4,127]\n        \n        fig =plt.figure()\n        ax1 = plt.subplot(1,3,1)\n        ax1.set_xticks(default_x_ticks, time)\n        ax1.set_yticklabels([])\n        ax1.imshow(im, cmap='viridis')\n        \n        ax2 = plt.subplot(1,3,2)\n        ax2.set_xticks(default_x_ticks, time)\n        ax2.set_yticklabels([])\n        ax2.imshow(heatmap, cmap='jet')\n        \n        ax3 = plt.subplot(1,3,3)\n        ax3.set_xticks(default_x_ticks, time)\n        ax3.set_yticklabels([])\n        # 以 0.6 透明度繪製原始影像\n        ax3.imshow(im, cmap='viridis', alpha=0.7)\n        # 以 0.4 透明度繪製熱力圖\n        ax3.imshow(heatmap, cmap='jet', alpha=0.3)\n        \n        if(pred_class==1):\n            if(original_labels[i]==1):\n                 fig.suptitle('Groundtruth: REM/ Prediction: REM')\n            elif(original_labels[i]==0):\n                 fig.suptitle('Groundtruth: NREM/ Prediction: REM')\n        elif(pred_class==0):\n            if(original_labels[i]==1):\n                 fig.suptitle('Groundtruth: REM/ Prediction: NREM')\n            elif(original_labels[i]==0):\n                 fig.suptitle('Groundtruth: NREM/ Prediction: NREM')\n#         plt.colorbar(ax2.add_collection(heatmap_original),ax=ax2)\n        \n        plt.show()\n","metadata":{"id":"cqARIhN8RsqR","outputId":"3c538f16-4073-4dd6-f32c-f4c4e936d6de","execution":{"iopub.status.busy":"2023-11-14T08:20:01.867206Z","iopub.execute_input":"2023-11-14T08:20:01.868216Z","iopub.status.idle":"2023-11-14T08:20:20.988702Z","shell.execute_reply.started":"2023-11-14T08:20:01.868165Z","shell.execute_reply":"2023-11-14T08:20:20.987611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" plt.figure()\nplt.subplot(1,2,1)\nplt.imshow(original_preprocess_images[i], cmap='viridis')\nplt.subplot(1,2,2)\nplt.imshow(heatmap, cmap='jet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:37:37.366310Z","iopub.execute_input":"2023-10-21T04:37:37.367091Z","iopub.status.idle":"2023-10-21T04:37:37.651828Z","shell.execute_reply.started":"2023-10-21T04:37:37.367057Z","shell.execute_reply":"2023-10-21T04:37:37.650618Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def processing_image(img):\n    # # 讀取影像為 PIL 影像\n    # img = image.load_img(img_path, target_size=(224, 224))\n    \n    # # 轉換 PIL 影像為 nparray\n    # x = image.img_to_array(img)\n    \n    # 加上一個 batch size，例如轉換 (224, 224, 3) 為 （1, 224, 224, 3) \n    img = np.expand_dims(img, axis=0)\n    \n    x = np.log(img + 1e-10)\n#     x = preprocess_resnet50(img)\n    \n    return x","metadata":{"id":"tUAF6QoAFAjJ","execution":{"iopub.status.busy":"2023-07-20T13:58:39.499568Z","iopub.execute_input":"2023-07-20T13:58:39.500793Z","iopub.status.idle":"2023-07-20T13:58:39.507322Z","shell.execute_reply.started":"2023-07-20T13:58:39.500738Z","shell.execute_reply":"2023-07-20T13:58:39.506312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gradcam(model, x):\n    # 取得影像的分類類別\n    preds = model.predict(x)\n    pred_class = np.argmax(preds)\n    \n    # # 取得影像分類名稱\n    # pred_class_name = imagenet_utils.decode_predictions(preds)[0][0][1]\n    \n    # 預測分類的輸出向量\n    pred_output = model.output[:, pred_class]\n    \n    # 最後一層 convolution layer 輸出的 feature map\n    # ResNet 的最後一層 convolution layer\n    last_conv_layer = model.get_layer(index=9)\n    \n    # 求得分類的神經元對於最後一層 convolution layer 的梯度\n    grads = K.gradients(pred_output, last_conv_layer.output)[0]\n    \n    # 求得針對每個 feature map 的梯度加總\n    pooled_grads = K.sum(grads, axis=(0, 1, 2))\n    \n    # K.function() 讓我們可以藉由輸入影像至 `model.input` 得到 `pooled_grads` 與\n    # `last_conv_layer[0]` 的輸出值，像似在 Tensorflow 中定義計算圖後使用 feed_dict\n    # 的方式。\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    \n    # 傳入影像矩陣 x，並得到分類對 feature map 的梯度與最後一層 convolution layer 的 \n    # feature map\n    pooled_grads_value, conv_layer_output_value = iterate([x])\n    \n    # 將 feature map 乘以權重，等於該 feature map 中的某些區域對於該分類的重要性\n    for i in range(pooled_grads_value.shape[0]):\n        conv_layer_output_value[:, :, i] *= (pooled_grads_value[i])\n        \n    # 計算 feature map 的 channel-wise 加總\n    heatmap = np.sum(conv_layer_output_value, axis=-1)\n    \n    return heatmap, pred_class","metadata":{"id":"qFV1SytPFGOU","execution":{"iopub.status.busy":"2023-07-20T13:58:41.907132Z","iopub.execute_input":"2023-07-20T13:58:41.907516Z","iopub.status.idle":"2023-07-20T13:58:41.917966Z","shell.execute_reply.started":"2023-07-20T13:58:41.907483Z","shell.execute_reply":"2023-07-20T13:58:41.916589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_heatmap(heatmap, img, pred_class, REM_or_NREM, fig_name):\n    # ReLU\n    heatmap = np.maximum(heatmap, 0)\n    \n    # 正規化\n    heatmap /= np.max(heatmap)\n    \n    # # 讀取影像\n    # img = cv2.imread(img_path)\n    \n    fig, ax = plt.subplots()\n    \n    im = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), (img.shape[1], img.shape[0]))\n\n    # 拉伸 heatmap\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    heatmap = np.uint8(255 * heatmap)\n    \n    # 以 0.6 透明度繪製原始影像\n    ax.imshow(im, alpha=0.6)\n    \n    # 以 0.4 透明度繪製熱力圖\n    ax.imshow(heatmap, cmap='jet', alpha=0.4)\n    \n    \n    plt.title(str(pred_class))\n    \n    # Save figure\n    figure_name = r'/content/drive/MyDrive/DREAMS_data/SMOTE_5s_based_regenerated_balanced_data/CAM_SMOTE_with_without_REMs_phase_shift_80%_overlap/'+ REM_or_NREM + '/' + fig_name\n    plt.axis('off')\n    plt.savefig(figure_name)\n\n    plt.show()\n\n    ","metadata":{"id":"3he7T5wwFLo0","execution":{"iopub.status.busy":"2023-07-20T13:58:50.677638Z","iopub.execute_input":"2023-07-20T13:58:50.678621Z","iopub.status.idle":"2023-07-20T13:58:50.689721Z","shell.execute_reply.started":"2023-07-20T13:58:50.678578Z","shell.execute_reply":"2023-07-20T13:58:50.688610Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot specific Grad-CAM result\nchosen = 26\nheatmap, pred_class_name = gradcam(model, test_images[chosen:chosen+1,:,:,:])\nimg = test_images[chosen,:,:,:]\nheatmap = np.maximum(heatmap, 0)\n# 正規化\nheatmap /= np.max(heatmap)\nfig, ax = plt.subplots()\nim = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), (img.shape[1], img.shape[0]))\n# 拉伸 heatmap\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\n\n\n\n# 分別plot \n# 偽坐標軸(只顯示座標，不scaling)\n@plt.FuncFormatter\ndef fake_log(x, pos):\n    'The two args are the value and tick position'\n    return r'$2^{%d}$' % (x)\n# input image\nax.imshow(im, extent=[0, 5, 0, 6], aspect=5/6, cmap='viridis')\n# heat map\n# ax.imshow(heatmap, extent=[0, 5, 0, 6], aspect=5/6, cmap='jet')\n\n# 偽坐標軸(只顯示座標，不scaling)\nax.yaxis.set_major_formatter(fake_log)\n\nplt.ylabel('Frequency (Hz)')\nplt.xlabel('Time (s)')\nplt.show()\n\n\n\n\n#疊合\n# # 偽坐標軸(只顯示座標，不scaling)\n# @plt.FuncFormatter\n# def fake_log(x, pos):\n#     'The two args are the value and tick position'\n#     return r'$2^{%d}$' % (x)\n\n# # 以 0.6 透明度繪製原始影像\n# ax.imshow(im, extent=[0, 5, 0, 6], aspect=5/6, alpha=0.6)\n# # 以 0.4 透明度繪製熱力圖\n# ax.imshow(heatmap, extent=[0, 5, 0, 6], aspect=5/6, cmap='jet', alpha=0.4)\n\n# # 偽坐標軸(只顯示座標，不scaling)\n# ax.yaxis.set_major_formatter(fake_log)\n\n# plt.ylabel('Frequency (Hz)')\n# plt.xlabel('Time (s)')\n# plt.show()","metadata":{"id":"vWBg3U7YzR22","outputId":"3cef71a7-d125-477d-a227-aef4f7b0b36d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show wrong classified plot\n# pred = model.predict(preprocess_images)\n# pred = np.argmax(pred, axis = 1)\n\nfor q in range(0, len(labels_int)):\n  # 取得影像的分類類別\n  img = processing_image(images[q,:,:,:])\n  preds = model.predict(img)\n  pred_class = np.argmax(preds)\n\n  if((int(pred_class))!= (int(labels_int[q]))): \n    print(q)\n    # # 取得影像分類名稱\n    # pred_class_name = imagenet_utils.decode_predictions(preds)[0][0][1]\n      \n    # 預測分類的輸出向量\n    pred_output = model.output[:, pred_class]\n      \n    # 最後一層 convolution layer 輸出的 feature map\n    # ResNet 的最後一層 convolution layer\n    last_conv_layer = model.get_layer(index=9)\n      \n    # 求得分類的神經元對於最後一層 convolution layer 的梯度\n    grads = K.gradients(pred_output, last_conv_layer.output)[0]\n      \n    # 求得針對每個 feature map 的梯度加總\n    pooled_grads = K.sum(grads, axis=(0, 1, 2))\n      \n    # K.function() 讓我們可以藉由輸入影像至 `model.input` 得到 `pooled_grads` 與\n    # `last_conv_layer[0]` 的輸出值，像似在 Tensorflow 中定義計算圖後使用 feed_dict\n    # 的方式。\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n      \n    # 傳入影像矩陣 x，並得到分類對 feature map 的梯度與最後一層 convolution layer 的 \n    # feature map\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n      \n    # 將 feature map 乘以權重，等於該 feature map 中的某些區域對於該分類的重要性\n    for i in range(pooled_grads_value.shape[0]):\n        conv_layer_output_value[:, :, i] *= (pooled_grads_value[i])\n          \n    # 計算 feature map 的 channel-wise 加總\n    heatmap = np.sum(conv_layer_output_value, axis=-1)\n\n\n\n    # ReLU\n    heatmap = np.maximum(heatmap, 0)\n      \n    # 正規化\n    heatmap /= np.max(heatmap)\n\n    print(int(labels_int[q]), \" -> \", int(pred_class))\n      \n    # # 讀取影像\n    # img = cv2.imread(img_path)\n    \n    im = cv2.resize(cv2.cvtColor(images[q,:,:,:], cv2.COLOR_BGR2RGB), (images[q,:,:,:].shape[1], images[q,:,:,:].shape[0]))\n    \n    # 拉伸 heatmap\n    heatmap = cv2.resize(heatmap, (images[q,:,:,:].shape[1], images[q,:,:,:].shape[0]))\n\n    heatmap = np.uint8(255 * heatmap)\n\n    # 分別plot \n    plt.figure()\n    plt.subplot(1,2,1)\n    plt.imshow(im, cmap='viridis')\n    plt.subplot(1,2,2)\n    plt.imshow(heatmap, cmap='jet')\n    plt.show()\n    \n    \n    # # 疊在一起plot\n    # plt.imshow(im, alpha=0.7)\n    # plt.imshow(heatmap, cmap='jet', alpha=0.3)\n    # plt.show()\n\n\n\n","metadata":{"id":"cgsK2y8378vL","outputId":"9a0b3634-c2d5-47d1-d004-cb1c4fafd57f","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grad-CAM 主程式\nfor index,name in enumerate(REM_image_name):\n  img = processing_image(REM_images[index,:,:])\n\n  heatmap, pred_class_name = gradcam(model, img)\n\n  plot_heatmap(heatmap, REM_images[index,:,:], pred_class_name, 'REMs', name)\n\nfor index,name in enumerate(NREM_image_name):\n  img = processing_image(NREM_images[index,:,:])\n\n  heatmap, pred_class_name = gradcam(model, img)\n\n  plot_heatmap(heatmap, NREM_images[index,:,:], pred_class_name, 'NREMs', name)","metadata":{"id":"YzyMIgRIHKqw","outputId":"d883278e-021b-432e-cd7b-84e2795ecc84"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(200,220):\n  # img = processing_image(images[i,:,:,:])\n\n  heatmap, pred_class_name = gradcam(model, test_images[i:i+1,:,:,:])\n\n  if(pred_class_name==1):\n    if(test_labels[i]==1):\n      plot_heatmap(heatmap, test_images[i,:,:,:], 'Prediction: REMs', 'REMs', str(i) + '(Groundtruth: REMs)')\n    elif(test_labels[i]==0):\n      plot_heatmap(heatmap, test_images[i,:,:,:], 'Prediction: REMs', 'REMs', str(i) + '(Groundtruth: NREMs)')\n  elif(pred_class_name==0):\n    if(test_labels[i]==1):\n      plot_heatmap(heatmap, test_images[i,:,:,:], 'Prediction: NREMs', 'NREMs', str(i) + '(Groundtruth: REMs)')\n    elif(test_labels[i]==0):\n      plot_heatmap(heatmap, test_images[i,:,:,:], 'Prediction: NREMs', 'NREMs', str(i) + '(Groundtruth: NREMs)')","metadata":{"id":"S4ukLotdnsfi","outputId":"82599801-23a6-47f5-d903-739e49d73741","execution":{"iopub.status.busy":"2023-07-20T13:58:58.718905Z","iopub.execute_input":"2023-07-20T13:58:58.719323Z","iopub.status.idle":"2023-07-20T13:58:59.733002Z","shell.execute_reply.started":"2023-07-20T13:58:58.719287Z","shell.execute_reply":"2023-07-20T13:58:59.731659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_labels)","metadata":{"id":"PhcPRw6dvr6h"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"id":"xp8JMqazx6qg","execution":{"iopub.status.busy":"2023-05-30T00:51:36.464615Z","iopub.execute_input":"2023-05-30T00:51:36.465102Z","iopub.status.idle":"2023-05-30T00:51:36.484178Z","shell.execute_reply.started":"2023-05-30T00:51:36.465011Z","shell.execute_reply":"2023-05-30T00:51:36.482716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 透過convert_to_labels 將one-hot-encode的資料轉成label\ndef convert_to_labels(X):\n  return np.argmax(X, axis=1)","metadata":{"id":"AluTjLtH2yA5","execution":{"iopub.status.busy":"2023-05-30T00:51:39.733634Z","iopub.execute_input":"2023-05-30T00:51:39.734102Z","iopub.status.idle":"2023-05-30T00:51:39.743857Z","shell.execute_reply.started":"2023-05-30T00:51:39.734020Z","shell.execute_reply":"2023-05-30T00:51:39.742924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Generate confudion matrix(main function)\ny_predict = model.predict(test_images, batch_size=None, verbose=0, steps=None)\nprint(y_predict)\ny_pred = convert_to_labels(y_predict)\nprint(y_pred)\ny_true = test_labels\ntarget_names = ['No_REMs','With_REMs']\nprint(classification_report(y_true, y_pred, target_names=target_names))\nprint (\"**************************************************************\")\n\nplt.figure()\ncnf_matrix = confusion_matrix(y_true, y_pred)\nplot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True, title='confusion matrix')\n\nplt.show()","metadata":{"id":"RwJWEDtWy0Cn","outputId":"89a0bf06-76ba-4ce6-a783-b415330f6e7e","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Spectrogram CNN model (Stage2-with-REMs)\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\nfrom keras.callbacks import LearningRateScheduler\n\n###########################################\n# DEFINE A STEPPED LEARNING RATE SCHEDULE #\n###########################################\n\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))\n\nmodel_REMs = tf.keras.models.Sequential()\nmodel_REMs.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\nmodel_REMs.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.Flatten())\nmodel_REMs.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_REMs.add(tf.keras.layers.Dropout(0.5))\nmodel_REMs.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n\n# Compile model\nmodel_REMs.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.RMSprop(),\n    metrics=['accuracy'],\n)\n\nhistory = model_REMs.fit(REMs_model_train_images, REMs_model_train_labels, epochs=15, batch_size=32, callbacks=[lr_sched], \n                    validation_data=(REMs_model_valid_images, REMs_model_valid_labels))","metadata":{"id":"ggIqBp4V09yP","outputId":"b193cd3a-77b2-4e47-b121-df9aec64983a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Spectrogram CNN model (Stage2-withour-REMs)\nIMAGE_HEIGHT = img_size[0]\nIMAGE_WIDTH = img_size[1]\nN_CHANNELS = 3\nN_CLASSES = 2\n\nfrom keras.callbacks import LearningRateScheduler\n\n###########################################\n# DEFINE A STEPPED LEARNING RATE SCHEDULE #\n###########################################\n\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch / 2)))\n\nmodel_no_REMs = tf.keras.models.Sequential()\nmodel_no_REMs.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\nmodel_no_REMs.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.Flatten())\nmodel_no_REMs.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel_no_REMs.add(tf.keras.layers.BatchNormalization())\nmodel_no_REMs.add(tf.keras.layers.Dropout(0.5))\nmodel_no_REMs.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n\n# Compile model\nmodel_no_REMs.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.RMSprop(),\n    metrics=['accuracy'],\n)\n\nhistory = model_no_REMs.fit(NREMs_model_train_images, NREMs_model_train_labels, epochs=15, batch_size=32, callbacks=[lr_sched], \n                    validation_data=(NREMs_model_valid_images, NREMs_model_valid_labels))","metadata":{"id":"J8wlbUE_6vCA","outputId":"190a51d7-5185-4cd5-84ec-7050cebb3fde"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stage2_1_input_images = np.zeros((1,256,256,3), dtype=np.double)\nstage2_1_input_labels = np.zeros((1,), dtype=int)\nstage2_2_input_images = np.zeros((1,256,256,3), dtype=np.double)\nstage2_2_input_labels = np.zeros((1,), dtype=int)\ncount1 = 0\ncount0 = 0\nfor i in range(len(y_pred)):\n  if(y_pred[i]==1):\n    stage2_1_input_images = np.append(stage2_1_input_images, test_images[i:i+1,:,:,:],axis=0)\n    stage2_1_input_labels = np.append(stage2_1_input_labels, test_labels[i:i+1],axis=0)\n    count1 +=1\n  elif(y_pred[i]==0):\n    stage2_2_input_images = np.append(stage2_2_input_images, test_images[i:i+1,:,:,:],axis=0)\n    stage2_2_input_labels = np.append(stage2_2_input_labels, test_labels[i:i+1],axis=0)\n    count0 += 1\n\nstage2_1_input_images = stage2_1_input_images[1:,:,:,:]\nstage2_1_input_labels = stage2_1_input_labels[1:]\nstage2_2_input_images = stage2_2_input_images[1:,:,:,:]\nstage2_2_input_labels = stage2_2_input_labels[1:]\n\n","metadata":{"id":"GvxrdhYhen7p"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from numpy.lib.function_base import append\n#  Generate confudion matrix for stage2-with REMs classification(main function)\ny_predict_REMs = model.predict(stage2_1_input_images, batch_size=None, verbose=0, steps=None)\ny_predict_no_REMs = model.predict(stage2_2_input_images, batch_size=None, verbose=0, steps=None)\n# print(y_predict_REMs)\n# print(y_predict_no_REMs)\ny_pred_REMs = convert_to_labels(y_predict_REMs)\ny_pred_no_REMs = convert_to_labels(y_predict_no_REMs)\n# print(y_pred_REMs)\n# print(y_pred_no_REMs)\ny_pred_final = np.append(y_pred_REMs,y_pred_no_REMs,axis=0)\n# print(y_pred_final)\ny_true_REMs = stage2_1_input_labels\ny_true_no_REMs = stage2_2_input_labels\ny_true_final = np.append(y_true_REMs, y_true_no_REMs,axis=0)\ntarget_names = ['NREMs','REM']\nprint(classification_report(y_true_final, y_pred_final, target_names=target_names))\nprint (\"**************************************************************\")\n\nplt.figure()\ncnf_matrix = confusion_matrix(y_true_final, y_pred_final)\nplot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True, title='confusion matrix')\n\nplt.show()","metadata":{"id":"MgOcv_bp2P7s","outputId":"beca96a2-f63b-4a2b-e24d-5dcf575a1cdb","execution":{"iopub.status.busy":"2023-07-24T01:30:07.923482Z","iopub.execute_input":"2023-07-24T01:30:07.924526Z","iopub.status.idle":"2023-07-24T01:30:07.955608Z","shell.execute_reply.started":"2023-07-24T01:30:07.924488Z","shell.execute_reply":"2023-07-24T01:30:07.954236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# release the memory\ndel preprocess_images,images,train_images,train_labels,valid_images,valid_labels,test_images,test_labels\ndel model","metadata":{"id":"xjeiTk01YUKz"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"r82h60nZ52es"}}]}